
name: 🧪 End-to-End Tests

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  schedule:
    # Run E2E tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security
      skip_slow:
        description: 'Skip slow tests'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  actions: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  TESTING: 'true'
  LOG_LEVEL: 'DEBUG'

jobs:
  test-matrix:
    name: 🧪 Test Matrix Setup
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    
    steps:
    - name: 📋 Set Test Matrix
      id: set-matrix
      run: |
        if [ "${{ github.event.inputs.test_type }}" = "performance" ]; then
          echo 'matrix={"test_type":["performance"],"os":["ubuntu-latest"]}' >> $GITHUB_OUTPUT
        elif [ "${{ github.event.inputs.test_type }}" = "security" ]; then
          echo 'matrix={"test_type":["security"],"os":["ubuntu-latest"]}' >> $GITHUB_OUTPUT
        elif [ "${{ github.event.inputs.test_type }}" = "e2e" ]; then
          echo 'matrix={"test_type":["e2e"],"os":["ubuntu-latest","windows-latest","macos-latest"]}' >> $GITHUB_OUTPUT
        else
          echo 'matrix={"test_type":["unit","integration","e2e"],"os":["ubuntu-latest","windows-latest"]}' >> $GITHUB_OUTPUT
        fi

  unit-tests:
    name: 🔬 Unit Tests
    runs-on: ${{ matrix.os }}
    needs: test-matrix
    if: contains(fromJson(needs.test-matrix.outputs.matrix).test_type, 'unit') || github.event.inputs.test_type == 'all'
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix on non-Linux
          - os: windows-latest
            python-version: '3.9'
          - os: windows-latest
            python-version: '3.10'
          - os: macos-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.10'
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4

    - name: 🐍 Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-xdist pytest-benchmark pytest-timeout pytest-html

    - name: 🔬 Run Unit Tests
      run: |
        pytest tests/ -m "unit and not slow" \
          --maxfail=5 \
          --tb=short \
          -n auto \
          --dist=worksteal \
          --junit-xml=data/tests/unit-junit-${{ matrix.os }}-${{ matrix.python-version }}.xml

    - name: 📊 Upload Unit Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          data/tests/
          data/coverage/
        retention-days: 30

  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ${{ matrix.os }}
    needs: test-matrix
    if: contains(fromJson(needs.test-matrix.outputs.matrix).test_type, 'integration') || github.event.inputs.test_type == 'all'
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-xdist pytest-timeout
        
    - name: 🔗 Run Integration Tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: sqlite:///test.db
      run: |
        pytest tests/ -m "integration" \
          --maxfail=3 \
          --tb=short \
          --timeout=600 \
          --junit-xml=data/tests/integration-junit-${{ matrix.os }}.xml
          
    - name: 📊 Upload Integration Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results-${{ matrix.os }}
        path: |
          data/tests/
          data/logs/
        retention-days: 30

  e2e-tests:
    name: 🎯 End-to-End Tests
    runs-on: ${{ matrix.os }}
    needs: test-matrix
    if: contains(fromJson(needs.test-matrix.outputs.matrix).test_type, 'e2e') || github.event.inputs.test_type == 'all'
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-asyncio pytest-timeout pytest-mock
        
    - name: 🎯 Run End-to-End Tests
      env:
        SKIP_NETWORK_TESTS: ${{ github.event.inputs.skip_slow || 'false' }}
        E2E_TEST_TIMEOUT: 1800  # 30 minutes
      run: |
        python tests/test_e2e.py
        
    - name: 🧪 Run E2E Test Suite with pytest
      run: |
        pytest tests/test_e2e.py -v \
          --tb=short \
          --timeout=1800 \
          --junit-xml=data/tests/e2e-junit-${{ matrix.os }}.xml
          
    - name: 📊 Upload E2E Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results-${{ matrix.os }}
        path: |
          data/tests/
          data/logs/
          data/coverage/
        retention-days: 30

  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'all'
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install pytest-benchmark pytest-memray memory-profiler
        
    - name: ⚡ Run Performance Tests
      run: |
        pytest tests/ -m "performance" \
          --benchmark-only \
          --benchmark-json=data/tests/benchmark.json \
          --benchmark-histogram=data/tests/histogram \
          --benchmark-sort=mean \
          --tb=short \
          --junit-xml=data/tests/performance-junit.xml
          
    - name: 📈 Generate Performance Report
      run: |
        python -c "
        import json
        import sys
        
        try:
            with open('data/tests/benchmark.json') as f:
                data = json.load(f)
                
            print('# ⚡ Performance Test Results')
            print()
            
            for benchmark in data.get('benchmarks', []):
                name = benchmark['name']
                stats = benchmark['stats']
                print(f'## {name}')
                print(f'- **Mean:** {stats[\"mean\"]:.6f}s')
                print(f'- **Min:** {stats[\"min\"]:.6f}s')
                print(f'- **Max:** {stats[\"max\"]:.6f}s')
                print(f'- **StdDev:** {stats[\"stddev\"]:.6f}s')
                print()
        except FileNotFoundError:
            print('No benchmark results found')
        " > data/tests/performance-report.md
        
    - name: 📊 Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          data/tests/benchmark.json
          data/tests/histogram/
          data/tests/performance-report.md
        retention-days: 30

  security-tests:
    name: 🔒 Security Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'security' || github.event.inputs.test_type == 'all'
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install bandit safety semgrep
        
    - name: 🔒 Run Security Tests
      run: |
        pytest tests/ -m "security" \
          --tb=short \
          --junit-xml=data/tests/security-junit.xml
          
    - name: 🛡️ Run Bandit Security Scan
      run: |
        bandit -r src/ -f json -o data/tests/bandit-report.json || true
        
    - name: 🔍 Run Safety Check
      run: |
        safety check --json --output data/tests/safety-report.json || true
        
    - name: 📊 Upload Security Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-test-results
        path: |
          data/tests/security-junit.xml
          data/tests/bandit-report.json
          data/tests/safety-report.json
        retention-days: 30

  test-report:
    name: 📊 Generate Test Report
    permissions:
      contents: read
      pull-requests: write
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v5
      
    - name: 📊 Download All Test Results
      uses: actions/download-artifact@v5
      with:
        path: test-results/
        
    - name: 📋 Generate Comprehensive Report
      run: |
        python -c "
        import json
        import os
        from pathlib import Path
        from datetime import datetime
        
        report = f'''# 🧪 Comprehensive Test Report
        
        **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}  
        **Workflow:** {os.environ.get('GITHUB_WORKFLOW', 'Unknown')}  
        **Run ID:** {os.environ.get('GITHUB_RUN_ID', 'Unknown')}
        
        ## 📊 Test Results Summary
        
        '''
        
        # Scan for test result files
        results_dir = Path('test-results')
        if results_dir.exists():
            for category in ['unit', 'integration', 'e2e', 'performance', 'security']:
                category_files = list(results_dir.glob(f'**/{category}*'))
                if category_files:
                    report += f'### {category.title()} Tests\n'
                    report += f'- **Files Found:** {len(category_files)}\n'
                    for file in category_files[:5]:  # Show first 5 files
                        report += f'  - {file.name}\n'
                    if len(category_files) > 5:
                        report += f'  - ... and {len(category_files) - 5} more\n'
                    report += '\n'
        
        # Test coverage summary
        report += '''
        ## 📈 Coverage Summary
        
        Coverage reports are available in the artifacts.
        
        ## 🔗 Useful Links
        
        - [GitHub Actions Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
        - [Test Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}#artifacts)
        
        '''
        
        with open('comprehensive-test-report.md', 'w') as f:
            f.write(report)
        "
        
    - name: 📤 Upload Comprehensive Report
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-test-report
        path: comprehensive-test-report.md
        retention-days: 30
        
    - name: 💬 Comment Test Results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          let report = '# 🧪 Test Results Summary\n\n';
          
          // Read comprehensive report if available
          try {
            const comprehensiveReport = fs.readFileSync('comprehensive-test-report.md', 'utf8');
            report += comprehensiveReport;
          } catch (error) {
            report += 'Comprehensive report not available.\n';
          }
          
          report += `\n**Workflow Run:** [View Details](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;
          
          // Comment on PR
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });

  cleanup:
    name: 🧹 Cleanup
    permissions:
      actions: read
    runs-on: ubuntu-latest
    needs: [test-report]
    if: always()
    
    steps:
    - name: 🧹 Cleanup Old Artifacts
      uses: actions/github-script@v7
      with:
        script: |
          const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId,
          });
          
          console.log(`Found ${artifacts.data.artifacts.length} artifacts`);
          
          // Keep only the latest 10 artifacts per type
          const artifactsByType = {};
          artifacts.data.artifacts.forEach(artifact => {
            const type = artifact.name.split('-')[0];
            if (!artifactsByType[type]) {
              artifactsByType[type] = [];
            }
            artifactsByType[type].push(artifact);
          });
          
          // This is just logging for now
          Object.entries(artifactsByType).forEach(([type, arts]) => {
            console.log(`${type}: ${arts.length} artifacts`);
          });